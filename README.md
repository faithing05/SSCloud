# **SSCloud - Semantic Segmentation Cloud**

**SSCloud** — это проект, предназначенный для работы с семантической сегментацией изображений. Он включает в себя инструменты для полуавтоматической разметки данных с использованием [CVAT](https://github.com/cvat-ai/cvat) и последующего обучения нейросетевой архитектуры [Unet](https://github.com/milesial/Pytorch-UNet) на подготовленном датасете.

Этот репозиторий содержит все необходимые инструкции и скрипты для развертывания окружения, подготовки данных и запуска процессов обучения и инференса (применения модели).

## **Часть 1: Настройка локального сервера CVAT**

[CVAT (Computer Vision Annotation Tool)](https://github.com/cvat-ai/cvat) — это мощный инструмент для разметки изображений и видео. Следующие шаги помогут вам развернуть его локально с помощью Docker.

### **1. Клонирование репозитория CVAT**
Если у вас еще нет локальной копии репозитория CVAT, выполните следующую команду в терминале:
```bash
git clone https://github.com/cvat-ai/cvat
```

### **2. Переход в директорию проекта**
```bash
cd cvat
```

### **3. Сборка и запуск контейнеров**
Эта команда скачает последние образы CVAT и запустит все необходимые сервисы в фоновом режиме (флаг `-d`).
```bash
docker-compose up -d
```

### **4. Проверка статуса сервисов**
Подождите несколько минут, пока все компоненты запустятся. Убедиться, что все работает корректно, можно с помощью команды:
```bash
docker-compose ps
```

### **5. Создание суперпользователя**
Для доступа к интерфейсу CVAT необходимо создать учетную запись администратора. Сначала зайдите внутрь контейнера:
```bash
docker exec -it cvat_server bash
```
Затем, находясь внутри контейнера, выполните команду для создания пользователя и следуйте инструкциям в терминале:
```bash
python manage.py createsuperuser
```
После создания пользователя выйдите из контейнера:
```bash
exit
```

### **6. Доступ к веб-интерфейсу**
Чтобы узнать адрес, по которому доступен веб-интерфейс (обычно это `http://localhost:8080/`), и посмотреть логи сервиса, выполните:
```bash
docker logs cvat_server -f
```
Откройте указанный адрес в браузере. Для остановки просмотра логов нажмите `CTRL+C`.

### **7. Остановка и удаление контейнеров CVAT**
Когда вы закончите работу с CVAT, остановить все сервисы и удалить контейнеры можно командой:
```bash
docker-compose down
```

## **Часть 2: Сборка и запуск Docker-образа для сегментации**

Этот раздел описывает, как собрать и запустить основной Docker-образ проекта `SSCloud` для работы с моделями сегментации.

### **Сборка образа**
Команда для сборки Docker-образа с именем `s3d` из Dockerfile в текущей директории. Флаг `--no-cache` гарантирует, что образ будет собран с нуля.
```bash
docker build --no-cache -t s3d .
```

### **Запуск контейнера**
Запуск контейнера из образа `s3d`.
*   `--gpus all`: предоставляет контейнеру доступ ко всем GPU.
*   `-it`: запускает контейнер в интерактивном режиме.
*   `-p 8888:8888`: пробрасывает порт 8888 с хоста в контейнер (например, для Jupyter Notebook).
*   `-v "${PWD}:/app"`: монтирует текущую директорию в папку `/app` внутри контейнера.
```bash
docker run --gpus all -it -p 8888:8888 -v "${PWD}:/app" s3d
```

*(Примечание: В вашем файле было две одинаковые команды для запуска. Если вторая команда должна была отличаться, например, монтированием дополнительных путей для моделей, её стоит уточнить. Сейчас они идентичны)*.
```bash
docker run --gpus all -it --rm -p 8888:8888 -v "${PWD}:/app" s3d
```

## **Часть 3: Обучение и инференс модели Unet**

Этот раздел посвящен обучению архитектуры **Unet** на вашем датасете и последующему применению обученной модели для предсказания масок.

### **Подготовка данных**
Перед обучением необходимо подготовить датасет. Скрипт `convert_labelme.py` обрабатывает ваши панорамы и разметку:
*   Исходные изображения перемещаются в `train/data/imgs`.
*   Маски сегментации преобразуются в нужный формат и перемещаются в `train/data/masks`.

### **Запуск контейнера для Unet**

**Для обучения:**
Эта команда запускает контейнер `milesial/unet`, монтируя вашу папку с данными (`data`) внутрь контейнера.
*   `--shm-size=8g`: увеличивает размер разделяемой памяти, что может быть полезно при работе с большими объемами данных.
```bash
docker run --rm -it --gpus all --shm-size=8g --ulimit memlock=-1 -v "${PWD}/data:/app/data" milesial/unet
```

**Для инференса (применения модели):**
Эта команда также запускает контейнер, но монтирует папки для тестовых данных, вывода и моделей (`checkpoints`).```bash
docker run --rm -it --gpus all -v "${PWD}/data/test/input:/workspace/unet/test_input"  -v "${PWD}/data/test/output:/workspace/unet/test_output" -v "${PWD}/checkpoints:/workspace/unet/checkpoints" milesial/unet
```

### **Работа внутри контейнера**

**Запуск обучения:**
После входа в контейнер, запустите обучение модели.
*   `WANDB_MODE=offline`: отключает логирование в Weights & Biases.
*   `--epochs 50`: количество эпох обучения.
*   `--batch-size 1`: размер батча.
*   `--scale 0.1`: коэффициент масштабирования изображений.
*   `--classes 68`: количество классов сегментации.
```bash
WANDB_MODE=offline python train.py --epochs 50 --batch-size 1 --scale 0.1 --amp --classes 68
```
    
**Запуск инференса:**
Применение обученной модели для предсказания на одном изображении.
*   `-m checkpoints/checkpoint_epoch48.pth`: путь к файлу с весами модели.
*   `-i test_input/1_normals.jpg`: входное изображение.
*   `-o test_output/predicted_mask.png`: путь для сохранения результата.
```bash
python predict.py -m checkpoints/checkpoint_epoch48.pth -i test_input/1_normals.jpg -o test_output/predicted_mask.png --scale 0.1 --classes 68
```

### **Дополнительные скрипты (выполняются на хост-машине)**

**Конвертация тестового датасета:**
Для подготовки тестовых данных (изображений в `data/test/imgs` и масок в `data/test/masks`) в машиночитаемый формат, выполните в **PowerShell**:
```powershell
python _convert_labelme.py
```

**Визуализация результата:**
Для наглядного представления результата работы модели (инференса) используйте этот скрипт:
```powershell
python _visualize_prediction.py
```